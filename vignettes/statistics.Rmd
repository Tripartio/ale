---
title: "ALE-based statistics (experimental)"
author: "Chitu Okoli"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ALE-based statistics (experimental)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r libraries}
library(ale)
library(dplyr)  # for data manipulation
library(mgcv)  # for datasets and for gam function
```

## Example dataset

We will demonstrate ALE statistics using a dataset composed and transformed from the `mgcv` package. This package is required to create teh generalized additive model (GAM) that we will use for this demonstration. (Strictly speaking, the source datasets are in the `nlme` package, which is loaded automatically when we load the `mgcv` package.) Here is the code to generate the data that we will work with:

```{r data setup}
# Create and prepare the data
math <- 
  # Start with math achievement scores per student
  MathAchieve |> 
  as_tibble() |> 
  mutate(
    school = School |> as.character() |>  as.integer(),
    minority = Minority == 'Yes',
    female = Sex == 'Female'
  ) |> 
  # summarize the scores to give per-school values
  summarize(
    .by = school,
    minority_ratio = mean(minority),
    female_ratio = mean(female),
    math_avg = mean(MathAch),
  ) |> 
  # merge the summarized student data with the school data
  inner_join(
    MathAchSchool |> 
      mutate(school = School |> as.character() |>  as.integer()),
    by = c('school' = 'school')
  ) |> 
  mutate(
    public = Sector == 'Public',
    high_minority = HIMINTY == 1,
  ) |> 
  select(-School, -Sector, -HIMINTY) |> 
  rename(
    size = Size,
    academic_ratio = PRACAD,
    discrim = DISCLIM,
    mean_ses = MEANSES,
  ) |> 
  # Remove ID column for analysis
  select(-school) |> 
  select(
    math_avg, size, public, academic_ratio,
    female_ratio, mean_ses, minority_ratio, high_minority, discrim,
    everything()
  )

glimpse(math)
```

The structure has 160 rows, each of which refers to a school whose students have taken a mathematics achievement test. We describe the data here based on documentation from the `nlme` package but many details are not quite clear:

| variable       | format  | description                                                                                                                             |
|------------------------------|------------------|------------------------|
| math_avg       | double  | average mathematics achievement scores of all students in the school                                                                    |
| size           | double  | the number of students in the school                                                                                                    |
| public         | logical | TRUE if the school is in the public sector; FALSE if in the Catholic sector                                                             |
| academic_ratio | double  | the percentage of students on the academic track                                                                                        |
| female_ratio   | double  | percentage of students in the school that are female                                                                                    |
| mean_ses       | double  | mean socioeconomic status for the students in the school (measurement is not quite clear)                                               |
| minority_ratio | double  | percentage of students that are members of a minority racial group                                                                      |
| high_minority  | logical | TRUE if the school has a high ratio of students of minority racial groups (unclear, but perhaps relative to the location of the school) |
| discrim        | double  | the "discrimination climate" (perhaps an indication of extent of racial discrimination in the school?)                                  |

The outcome variable that is the focus of our analysis is `math_avg`, the average mathematics achievement scores of all students in each school. Here are its descriptive statistics:

```{r y summary}
summary(math$math_avg)
```

## Full model bootstrap

Now we create a model and compute statistics on it. Because this is a relatively small model, we will carry out full model bootstrapping using the `model_bootstrap` function. We create a generalized additive model (GAM) so that we can capture non-linear relationships in the data.

By default, `model_bootstrap` runs 100 bootstrap iterations; this can be controlled with the `boot_it` argument. Bootstrapping is usually quite slow since the entire process is repeated that many times. The default of 100 should be sufficiently stable for model building, when you would want to run the bootstrapped algorithm several times and you do not want it to be too slow each time. For definitive conclusions, you could run 1,000 bootstraps or more to confirm the results of 100 bootstraps.

```{r model bootstrap, cache=TRUE}
mb_gam <- model_bootstrap(
  math, 
  'gam(
     math_avg ~ public + high_minority +
     s(size) + s(academic_ratio) + s(female_ratio) + s(mean_ses) + 
     s(minority_ratio) + s(discrim)
   )',
  # For the GAM model coefficients, show details of the non-smoothed (parametric) variables
  tidy_options = list(parametric = TRUE)
)
```

We can see the bootstrapped values of various overall model statistics by printing the `model_stats` element of the model bootstrap object:

```{r model_stats}
mb_gam$model_stats
```

The names of the columns follow the `broom` package conventions:

-   `name` is the specific overall model statistic described in the row.
-   `estimate` is the bootstrapped estimate of the statistic. It is the same as the bootstrap `mean` by default, though it can be set to the `median` with the `boot_centre` argument of `model_bootstrap`. Regardless, both the `mean` and `median` estimates are always returned. The `estimate` column is provided for convenience since that is a standard name in the `broom` package.
-   `conf.low` and `conf.high` are the lower and upper confidence intervals respectively. `model_bootstrap` defaults to a 95% confidence interval; this can be changed by setting the `boot_alpha` argument (the default is 0.05 for a 95% confidence interval).
-   `sd` is the standard deviation of the bootstrapped estimate.

Our focus, however, in this vignette is on the effects of individual variables. These are available in the `model_coefs` element of the model bootstrap object:

```{r model_coefs}
mb_gam$model_coefs
```

In this vignette, we cannot go into the details of how GAM models work, but we note that for our model illustration, only the details of the non-numeric variables are shown here. The numeric variables are handled with GAM smoothing functions, so they do not have regular regression coefficients.

The `ale` package uses bootstrap-based confidence intervals, not p-values, to determine statistical significance. Although they are not quite as simple to interpret as counting the number of stars next to a p-value, they are not that complicated, either. Based on the default 95% confidence intervals, a coefficient is statistically significant if `conf.low` and `conf.high` are both positive or both negative. When we filter on this criterion, we find that almost all the coefficients are statistically significant:

```{r model_coefs stat sig variables}
mb_gam$model_coefs |> 
  # filter is TRUE if conf.low and conf.high are both positive or both negative because
  # multiplying two numbers of the same sign results in a positive number.
  filter((conf.low * conf.high) > 0)
```

We see that neither `public` nor `high_minority` has an effect that is statistically significantly different from zero. A basic challenge, though, with models that are based on the general linear model (including GAM and almost all other statistical analyses) is that their coefficient significance compares the estimates with the null hypothesis that there is no effect. However, even if there is an effect, it might not be practically meaningful. ALE-based statistics are explicitly tailored to emphasize practical implications beyond the notion of "statistical significance".

First, let us display the ALE plots for each variable:

```{r all ALE plots, cache=TRUE, fig.width=7, fig.height=10}
gridExtra::grid.arrange(grobs = mb_gam$ale$plots, ncol = 2)
```

We can see that most variables seem to have some sort of mean effect across various values. However, our focus must be on the bootstrap intervals. Crucial to our interpretation is the middle grey bar that indicates the median Â± 2.5%, that is, the middle 5% of all average mathematics achievement scores (`math_avg`) values in the dataset. The idea is that if any predictor can do no better than influencing `math_avg` to fall within this middle bar, then it only has a minimal effect. For an effect to be considered statistically significant, there should be no overlap between the confidence regions of a predictor variable and the middle bar.

For categorical variables (`public` and `high_minority` above), the confidence error bars for all categories overlap the middle bars. The confidence error bars indicate two useful pieces of information to us. When we compare them to the middle bars, their overlap or lack thereof tells us about the practical significance of the category. When we compare the confidence bars of one category with those of others, it allows us to assess if the category has a statistically significant effect that is different from that of the other categories; this is equivalent to the regular interpretation of coefficients for GAM and other GLM models. In both cases, the error bars of the TRUE and FALSE categories overlap each other, indicating that there is no statistically significant difference between categories. This is the same conclusion that we get from the coefficient tables above. In addition, each error bar overlaps the middle bar, indicating that none of the effects is practically significant, either.

For numeric variables, the confidence regions overlap the middle bar for most of the domains of the predictor variables except for some regions that we will examine. The extreme points of each variable (except for `discrim` and `female_ratio`) are usually either slightly below or slightly above the middle bar, indicating that extreme values have the most extreme effects: math achievement increases with increasing school size, academic track ratio, and mean socioeconomic status, whereas it decreases with increasing minority ratio. The ratio of females and the discrimination climate both overlap the middle bar for the entirety of their domains, so any apparent trends are not supported by the data.

However, although ALE plots allow rapid and intuitive conclusions, it is often helpful to have numbers that give us these conclusions. Thus, we have developed a collection of statistics based on ALE tailored for intuitive interpretation.

## ALE data structures for categorical and numeric variables

To understand how ALE statistics are calculated, we must understand the structure of ALE data. Let's begin simple with a binary variable with just two categories, `public`:

```{r ale data for public}
mb_gam$ale$data$public
```

Here is the meaning of each column of `ale$data` for a categorical variable:

-   `ale_x`: the different categories that exist in the categorical variable.
-   `ale_n`: the number of rows for that category in the dataset provided to the function.
-   `ale_y`: the ALE function value calculated for that category. For bootstrapped ALE, this is the same as `ale_y_mean` by default or `ale_y_median` if the the `relative_y = 'median'` argument is specified.
-   `ale_y_lo` and `ale_y_hi`: the lower and upper confidence intervals for the bootstrapped `ale_y` value.

By default, the `ale` package centres ALE values on the median of the outcome variable; in our dataset, the median of all the schools' average mathematics achievement scores is `r math$math_avg |> median() |> round(1)`. With ALE centred on the median, the weighted sum of ALE y values (weighted on `ale_n`) above the median is approximately equal to the weighted sum of those below the median. So, in the ALE plots above, when you consider the number of instances indicated by the rug plots and category percentages, the average weighted ALE y approximately equals the median.

Here is the ALE data structure for a numeric variable, `academic_ratio`:

```{r ale data for academic_ratio}
mb_gam$ale$data$academic_ratio
```

The columns are the same as with a categorical variable, but the meaning of `ale_x` is different since there are no categories. To calculate ALE for numeric variables, the range of x values is divided into fixed intervals (by default 100, customizable with the `x_intervals` argument). If the x values have fewer than 100 distinct values in the data, then each distinct value becomes an ale_x interval. (This is often the case with smaller datasets like ours; here `academic_ratio` has only 65 distinct values.) If there are more than 100 distinct values, then the range is divided into 100 percentile groups. So, `ale_x` represents each of these x-variable intervals. The other columns mean the same thing as with categorical variables: `ale_n` is the number of rows of data in each `ale_x` interval and `ale_y` is the calculated ALE for that `ale_x` value.

## ALE statistics

With this understanding, let us explain the ALE statistics. They are accessible through the `ale$stats` element of the bootstrap result object, with multiple views. To focus on all the statistics for a specific variable, we can access the `ale$stats$by_term` element. Here are the statistics for the categorical `public`:

```{r ALE stats for public}
mb_gam$ale$stats$by_term$public
```

Let's look at the plot for `public` to help us understand what these statistics tell us.

```{r ALE plot for public}
mb_gam$ale$plots$public
```

Now, here are the ALE statistics for the numeric `academic_ratio`:

```{r ALE stats for academic_ratio}
mb_gam$ale$stats$by_term$academic_ratio
```

We can see that the same ALE statistics apply to categorical and numeric variables, even though their plots are quite different, as we see here:

```{r ALE plot for academic_ratio}
mb_gam$ale$plots$academic_ratio
```

Before we present the various ALE statistics we must reiterate the timeless reminder that correlation is not causation. So, none of the scores necessarily means that an x variable *causes* a certain effect on the y outcome; we can only say that the ALE statistics indicate associated or related variations between the two variables. 

### ALE statistics on the scale of the y outcome variable

#### ALE range (ALER)

The easiest ALE statistic to understand is the ALE range (ALER). It is simply the range from the minimum to the maximum of any `ale_y` value for that variable. Mathematically, that is

$$\mathrm{ALER}(\mathrm{ale\_y}) = \{ \min(\mathrm{ale\_y}), \max(\mathrm{ale\_y}) \}$$  
where $\mathrm{ale\_y}$ is the vector of ALE y values for a variable.

All the ALE statistics are centred on zero so that they are consistent regardless of if the user chooses to centre their plots on zero, the median, or the mean. Specifically,

-   `aler_min`: minimum of any `ale_y` value for the variable.
-   `aler_max`: maximum of any `ale_y` value for the variable.

ALER shows the extreme values of a variable's effect on the outcome. So, with an ALER of `r c(mb_gam$ale$stats$by_term$public$estimate['aler_min'], mb_gam$ale$stats$by_term$public$estimate['aler_max']) |> round(2)`, the minimum of any ALE value for `public` is `r mb_gam$ale$stats$by_term$public$estimate['aler_min'] |> round(2)` below the median (that corresponds to `public == TRUE`) and the maximum (`public == FALSE`) is `r mb_gam$ale$stats$by_term$public$estimate['aler_max'] |> round(2)` above the median. The ALER for `academic_ratio` is considerably broader with `r c(mb_gam$ale$stats$by_term$academic_ratio$estimate['aler_min'], mb_gam$ale$stats$by_term$academic_ratio$estimate['aler_max']) |> round(2)`.

The unit for ALER is the same unit as the outcome variable; in our case, that is `math_avg` ranging from `r round(min(math$math_avg, 2))` to `r round(max(math$math_avg, 2))`. No matter what the average ALE values might be, the ALER quickly shows the minimum and maximum effects of any value of the x variable on the y variable.

#### ALE deviation (ALED)

While the ALE range shows the most extreme effects a variable might have on the outcome, the ALE deviation indicates its average effect over its full domain of values. With the zero-centred ALE values, it is conceptually similar to the weighted mean absolute error (MAE) of the ALE y values. Mathematically, it is 

$$\mathrm{ALED}(\mathrm{ale\_y}, \mathrm{ale\_n}) = \frac{\sum_{i=1}^{k} \left| \mathrm{ale\_y}_i \times \mathrm{ale\_n}_i \right|}{\sum_{i=1}^{k} \mathrm{ale\_n}_i}$$ 
where $i$ is the index of $k$ ALE x intervals for the variable (for a categorical variable, this is the number of distinct categories), $\mathrm{ale\_y}_i$ is the ALE y value for the $i$th ALE x interval, and $\mathrm{ale\_n}_i$ is the number of rows of data in the $i$th ALE x interval.

Based on its ALED, we can say that the average effect on math scores of whether a school is in the public or Catholic sector is `r mb_gam$ale$stats$by_term$public$estimate['aled'] |> round(2)` (again, out of a range from `r round(min(math$math_avg, 2))` to `r round(max(math$math_avg, 2))`). The average effect for ratio of academic track students is slightly higher at `r mb_gam$ale$stats$by_term$academic_ratio$estimate['aled'] |> round(2)`

### Normalized ALE statistics

Since ALER and ALED scores are scaled on the range of y for a given dataset, these scores cannot be compared across datasets. Thus, we present normalized versions of each with intuitive, comparable values. For intuitive interpretation, we normalize the scores on the minimum, median, and maximum of any dataset. In principle, we divide the zero-centred y values in a dataset into two halves: the lower half from the 0th to the 50th percentile (the median) and the upper half from the 50th to the 100th percentile. (Note that the median is included in both halves). With zero-centred ALE y values, all negative and zero values are converted to their percentile score relative to the lower half of the original y values while all positive ALE y values are converted to their percentile score relative to the upper half. Technically, this percentile assignment is the empirical cumulative distribution function (ECDF) of each half. (Note that we choose to include the score of zero ALE y in the lower half because it is analogous to the 50th percentile of all values, which more intuitively belongs in the lower half of 100 percentiles.)

The transformed maximum ALE y is thus scaled as a percentile from 0 to 1. Its formula is

$$
\mathrm{norm\_ale\_y} = \begin{cases} 
ECDF_{y_{\geq 0}}(\mathrm{ale\_y}) & \text{if }\mathrm{ale\_y} > 0 \\
-ECDF_{y_{\leq 0}}(\mathrm{ale\_y}) & \text{otherwise}
\end{cases} 
$$ 
where
- $ECDF_{y_{\geq 0}}$ is the ECDF of the non-negative values in `y`.
- $-ECDF_{y_{\leq 0}}$ is the ECDF of the negative values in `y` after they have been inverted (multiplied by -1).

#### Normalized ALER (NALER)

Based on this normalization, we first have the normalized ALER (NALER), which scales the minimum and maximum ALE y values from 0 to 1, centred on 0.5:

$$
\mathrm{NALER}(\mathrm{y, ale\_y}) = \{ \frac{\min(\mathrm{norm\_ale\_y}) + 1}{2}, \frac{\max(\mathrm{norm\_ale\_y}) + 1}{2} \}
$$  

where $y$ is the full vector of y values in the original dataset, required to calculate $\mathrm{norm\_ale\_y}$.

The result of this transformation is that NALER values can be interpreted as percentiles with respect to the range of y about the median (0.5). `naler_min` is always less than 0.5 and `naler_max` is always greater than 0.5. Their numbers represent the limits of the effect of the x variable with units in percentile scores of y. So, with a NALER of `r c(mb_gam$ale$stats$by_term$public$estimate['naler_min'], mb_gam$ale$stats$by_term$public$estimate['naler_max']) |> round(2)`, the minimum of any ALE value for `public` (`public == TRUE`) shifts math scores to the `r (mb_gam$ale$stats$by_term$public$estimate['naler_min'] |> round(2)) * 100`th percentile of y values whereas the maximum (`public == FALSE`) shifts math scores to the `r (mb_gam$ale$stats$by_term$public$estimate['naler_max'] |> round(2)) * 100`th percentile. Academic track ratio has a NALER of `r c(mb_gam$ale$stats$by_term$academic_ratio$estimate['naler_min'], mb_gam$ale$stats$by_term$academic_ratio$estimate['naler_max']) |> round(2)`, ranging from the `r (mb_gam$ale$stats$by_term$academic_ratio$estimate['naler_min'] |> round(2)) * 100`th to the `r (mb_gam$ale$stats$by_term$academic_ratio$estimate['naler_max'] |> round(2)) * 100`th percentiles of math scores.

#### Normalized ALED (NALED)

The normalization of ALED scores applies the same ALED formula as before but on the normalized ALE values instead of on the original ALE y values:

$$
\mathrm{NALED}(y, \mathrm{ale\_y}, \mathrm{ale\_n}) = \mathrm{ALED}(\mathrm{norm\_ale\_y}, \mathrm{ale\_n})
$$

NALED produces a score that ranges from 0 to 1. It can be interpreted as the middle percentile range associated with the average variation of the x variable. So, the NALED of public school status of `r c(mb_gam$ale$stats$by_term$public$estimate['naled']) |> round(2)` indicates that its average variation with math scores is the same as that of the middle `r (mb_gam$ale$stats$by_term$public$estimate['naled'] |> round(2)) * 100`th percentile of scores. Academic ratio has a NALED of `r c(mb_gam$ale$stats$by_term$academic_ratio$estimate['naled']) |> round(2)`, associated with an average variation of the middle `r (mb_gam$ale$stats$by_term$academic_ratio$estimate['naled'] |> round(2)) * 100`% of scores.



